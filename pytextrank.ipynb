{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize News articles from main page of a news website\n",
    "## Spacy Library\n",
    "Initially considere using Flair but decided to switch to Spacy which is better in the sense that it is \"industrial strength\" and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Ideas and How the results can be used\n",
    "* pick a news outlet\n",
    "* filter news that mentions the target word or phrase  \n",
    "* summarize article to get gist of news "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sift through news articles using python library *newspaper3k*  \n",
    "https://newspaper.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build newspaper object \n",
    "wp_paper = newspaper.build('https://www.washingtonpost.com/', memoize_articles=False)\n",
    "print(len(wp_paper.articles)) # number of articles on the front page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get urls of all the articles and store in a list\n",
    "urls = []\n",
    "for article in wp_paper.articles:\n",
    "    urls.append(article.url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make a list of stopwords, import punctuation and add newline to it\n",
    "\n",
    "stopwords = list(STOP_WORDS)\n",
    "from string import punctuation\n",
    "punctuation = punctuation+ '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to do below command for a better trained dictionary en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for finding score of each word in a sentence, we use pytextrank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add PyTextRank to the spaCy pipeline (do it once only otherwise it gives error)\n",
    "nlp.add_pipe(\"textrank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.pipe_names # check if textrank got added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returnss list contains all articles on front page that mention target word\n",
    "def desired_articles(word):\n",
    "    texts  = []\n",
    "    for u in urls[:100]:\n",
    "        article = Article(u)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        if word in article.text:\n",
    "            texts.append(article.text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for cleaning sentences before applying textrank \n",
    "#that will help later when calculating score of sentences.\n",
    "\n",
    "def clean_text(t):\n",
    "    return re.sub(\"[^a-zA-Z\\.\\?\\!]\", \" \",t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes in set of sentences or a text block(t) and gives dictionay of score \n",
    "#of keyphrases\n",
    "def dict_kphrase(t):\n",
    "    dict_kp = dict() \n",
    "    doc = nlp(clean_text(t))\n",
    "    for phrase in doc._.phrases:\n",
    "        dict_kp[phrase.text]=phrase.rank\n",
    "    return dict_kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for spliting text into sentences we will use nltk\n",
    "import nltk\n",
    "#nltk.download('punkt') # one time execution\n",
    "\n",
    "#function that returns list of lists (tokenized sentences)\n",
    "def tokenize_sentences(t):\n",
    "    list_sentences = []\n",
    "    for sents in t:\n",
    "        list_sentences.append(nltk.sent_tokenize(clean_text(sents)))\n",
    "    return list_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that gives dictionary of score of each sentence using keyphrase scores. Takes in \n",
    "#list of sentences (sentences) and a dictionary (d) of scores of keyphrases of that text\n",
    "def score(sentences,d): #changed t to sentences for clarity\n",
    "    score_dict = dict()\n",
    "    for sent in sentences:\n",
    "        score_dict[sent] = 0\n",
    "        for kphrase in d.keys():\n",
    "            if kphrase in sent:\n",
    "                score_dict[sent]+= d[kphrase]\n",
    "    return score_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of articles:\n",
    "The idea is to consider n highest ranked sentences as summary of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain summary of articles that mention the desired word.\n",
    "def summary(word):\n",
    "    t = desired_articles(word)\n",
    "    list_sentences = tokenize_sentences(t) # list of lists\n",
    "    summary = []\n",
    "    for i in range(len(t)):\n",
    "        d = dict_kphrase(t[i])\n",
    "        score_dict = score(list_sentences[i],d)\n",
    "        ThreeHighest = nlargest(3, score_dict, key = score_dict.get)\n",
    "        summary.append(ThreeHighest)\n",
    "    return(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\"vaccine\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
